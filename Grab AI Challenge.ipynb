{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating stanford car classifier\n",
    "In this notebook I'm creating car classifier using the publicly available Stanford car dataset, which contains total of 196 classes. I'll be using a pre-trained resnet101 with transfer learning to train the model. All layers will be fine tuned. Only the last fully connected layer will be replaced and train.\n",
    "\n",
    "I used resnet101 because this is the best model I can run with the computer resources I have.\n",
    "\n",
    "Dataset (196 classes):\n",
    "\n",
    "Train dataset: 8144 images, with an average: 41.5 images per class. This will later on split into 90% train and 10% validation.\n",
    "\n",
    "Test dataset: 8041 images, with an average: 41.0 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "['train', 'test']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils import data as D\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from os import walk\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import os\n",
    "\n",
    "'''\n",
    "I need the filename of the image to be display in output result.\n",
    "So that evaluator can tell the result belongs to which image.\n",
    "The original ImageFolder class don't provide such function.\n",
    "So, I overide the ImageFolder class to return the image filename.\n",
    "'''\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data and transform**\n",
    "\n",
    "First, lets create some transforms for our data and load the train/validation and test data with labels from the folders.\n",
    "\n",
    "Here I use 300 x 300 images with random horizontal flip, random rotation and normalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the dataset directory for my Kaggle kernel. \n",
    "Please comment this line and uncomment the following line if you run it on your workstation.\n",
    "'''\n",
    "dataset_dir = \"../input/process-car2/process_car/\"\n",
    "\n",
    "'''\n",
    "This line is the dataset directory if you run it on your local workstation.\n",
    "'''\n",
    "#dataset_dir = \"\"\n",
    "\n",
    "train_tfms = transforms.Compose([transforms.Resize((300, 300)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomRotation(15),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_tfms = transforms.Compose([transforms.Resize((300, 300)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = ImageFolderWithPaths(root=dataset_dir+\"train\", transform = train_tfms)\n",
    "\n",
    "'''\n",
    "This part I load image from folder and then split 90% for training and 10% for validation.\n",
    "The label will be the sub folder of each image.\n",
    "'''\n",
    "train_len = int(0.9 * 8144)\n",
    "valid_len = 8144 - train_len\n",
    "train_dataset, valid_dataset = D.random_split(dataset, lengths=[train_len, valid_len])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 32, shuffle=True, num_workers = 2)\n",
    "\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size = 32, shuffle=True, num_workers = 2)\n",
    "\n",
    "test_dataset = ImageFolderWithPaths(root=dataset_dir+\"test\", transform = test_tfms)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 32, shuffle=False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training function**\n",
    "\n",
    "Here we train our model, after each epoch, we test the model on the test data to see how it's going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This part train resnet model with default 10 epoch.\n",
    "'''\n",
    "def train_model(model, criterion, optimizer, scheduler, n_epochs = 10):\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_accuracies = []\n",
    "    # set the model to train mode initially\n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # get the inputs and assign them to cuda\n",
    "            inputs, labels, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss/acc later\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "\n",
    "        epoch_duration = time.time()-since\n",
    "        epoch_loss = running_loss/len(trainloader)\n",
    "        epoch_acc = 100/32*running_correct/len(trainloader)\n",
    "        print(\"Epoch %s, duration: %d s, loss: %.4f, acc: %.4f\" % (epoch+1, epoch_duration, epoch_loss, epoch_acc))\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_acc)\n",
    "        \n",
    "        # switch the model to eval mode to evaluate on test data\n",
    "        model.eval()\n",
    "        test_acc = evaluate_model(model)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # re-set the model to train mode after validating\n",
    "        model.train()\n",
    "        scheduler.step(test_acc)\n",
    "        since = time.time()\n",
    "        \n",
    "    model.eval()\n",
    "    get_predict(model)\n",
    "    print('Finished Training')\n",
    "    model.train()\n",
    "    return model, losses, accuracies, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate on training data**\n",
    "\n",
    "This function is called out after each epoch of training on the training data. We then measure the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This part evaluation the model every epoch with validation dataset.\n",
    "'''\n",
    "def evaluate_model(model):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(validloader, 0):\n",
    "            images, labels, _ = data\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_ft(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_acc = 100.0 * correct / total\n",
    "    print('Accuracy of the network on the validation images: %.2f %%' % (\n",
    "        test_acc))\n",
    "    return test_acc\n",
    "\n",
    "'''\n",
    "This part will get the prediction result from the model.\n",
    "The result include confidence result and also the best confidence result.\n",
    "'''\n",
    "result = []\n",
    "actual = []\n",
    "result2 = []\n",
    "fname = []\n",
    "\n",
    "def get_predict(model):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, labels, fnames = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_ft(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            result.append(predicted.cpu().numpy())\n",
    "            \n",
    "            tmp_predict = outputs.data\n",
    "            predicted2 = torch.nn.functional.softmax(tmp_predict)\n",
    "            result2.append(predicted2.data.cpu().numpy())\n",
    "            actual.append(labels.cpu().numpy())\n",
    "            fname.append(fnames)\n",
    "            \n",
    "            #total += labels.size(0)\n",
    "            #correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    '''\n",
    "    test_acc = 100.0 * correct / total\n",
    "    print('Accuracy of the network on the testing images: %.2f %%' % (\n",
    "        test_acc))\n",
    "    print('Done getting result')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /tmp/.torch/models/resnet101-5d3b4d8f.pth\n",
      "178728960it [00:06, 26866835.12it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "I use pretrained resnet101 from pytorch.\n",
    "The pretrained model downloaded from https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\n",
    "If you already have the pretrained file (.pth), you can comment the first line and uncomment the second line to use it.\n",
    "Please make sure the pth file is at the same directory as this file.\n",
    "'''\n",
    "model_ft = models.resnet101(pretrained=True)\n",
    "#model_ft = torch.load(\"resnet101-5d3b4d8f.pth\")\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# replace the last fc layer with an untrained one (requires grad by default)\n",
    "model_ft.fc = nn.Linear(num_ftrs, 196)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "\"\"\"\n",
    "probably not the best metric to track, but we are tracking the training accuracy and measuring whether\n",
    "it increases by atleast 0.9 per epoch and if it hasn't increased by 0.9 reduce the lr by 0.1x.\n",
    "However in this model it did not benefit me.\n",
    "\"\"\"\n",
    "lrscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, threshold = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training with 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, duration: 131 s, loss: 3.7730, acc: 19.1576\n",
      "Accuracy of the network on the validation images: 35.71 %\n",
      "Epoch 2, duration: 130 s, loss: 1.4789, acc: 60.7609\n",
      "Accuracy of the network on the validation images: 60.86 %\n",
      "Epoch 3, duration: 130 s, loss: 0.8395, acc: 76.5217\n",
      "Accuracy of the network on the validation images: 70.31 %\n",
      "Epoch 4, duration: 130 s, loss: 0.5460, acc: 84.7690\n",
      "Accuracy of the network on the validation images: 78.65 %\n",
      "Epoch 5, duration: 130 s, loss: 0.3956, acc: 89.3478\n",
      "Accuracy of the network on the validation images: 83.93 %\n",
      "Epoch 6, duration: 130 s, loss: 0.2904, acc: 92.1467\n",
      "Accuracy of the network on the validation images: 84.17 %\n",
      "Epoch 7, duration: 130 s, loss: 0.2206, acc: 94.2391\n",
      "Accuracy of the network on the validation images: 85.89 %\n",
      "Epoch 8, duration: 131 s, loss: 0.1041, acc: 97.5951\n",
      "Accuracy of the network on the validation images: 91.66 %\n",
      "Epoch 9, duration: 130 s, loss: 0.0725, acc: 98.2473\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 10, duration: 131 s, loss: 0.0644, acc: 98.6413\n",
      "Accuracy of the network on the validation images: 92.76 %\n",
      "Epoch 11, duration: 131 s, loss: 0.0622, acc: 98.7500\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 12, duration: 131 s, loss: 0.0578, acc: 98.8179\n",
      "Accuracy of the network on the validation images: 92.02 %\n",
      "Epoch 13, duration: 131 s, loss: 0.0596, acc: 98.8587\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 14, duration: 131 s, loss: 0.0548, acc: 98.8587\n",
      "Accuracy of the network on the validation images: 92.27 %\n",
      "Epoch 15, duration: 131 s, loss: 0.0600, acc: 98.9130\n",
      "Accuracy of the network on the validation images: 92.27 %\n",
      "Epoch 16, duration: 131 s, loss: 0.0590, acc: 98.9946\n",
      "Accuracy of the network on the validation images: 91.78 %\n",
      "Epoch 17, duration: 131 s, loss: 0.0563, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 93.37 %\n",
      "Epoch 18, duration: 131 s, loss: 0.0508, acc: 98.9402\n",
      "Accuracy of the network on the validation images: 92.02 %\n",
      "Epoch 19, duration: 131 s, loss: 0.0527, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 20, duration: 131 s, loss: 0.0563, acc: 98.9538\n",
      "Accuracy of the network on the validation images: 92.15 %\n",
      "Epoch 21, duration: 131 s, loss: 0.0486, acc: 99.0353\n",
      "Accuracy of the network on the validation images: 92.76 %\n",
      "Epoch 22, duration: 131 s, loss: 0.0551, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 23, duration: 131 s, loss: 0.0544, acc: 99.1304\n",
      "Accuracy of the network on the validation images: 91.66 %\n",
      "Epoch 24, duration: 131 s, loss: 0.0563, acc: 99.1033\n",
      "Accuracy of the network on the validation images: 93.50 %\n",
      "Epoch 25, duration: 131 s, loss: 0.0511, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 91.90 %\n",
      "Epoch 26, duration: 131 s, loss: 0.0548, acc: 98.9946\n",
      "Accuracy of the network on the validation images: 93.50 %\n",
      "Epoch 27, duration: 131 s, loss: 0.0517, acc: 99.0761\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 28, duration: 131 s, loss: 0.0526, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 29, duration: 131 s, loss: 0.0503, acc: 98.9402\n",
      "Accuracy of the network on the validation images: 93.37 %\n",
      "Epoch 30, duration: 131 s, loss: 0.0618, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 31, duration: 131 s, loss: 0.0615, acc: 99.0353\n",
      "Accuracy of the network on the validation images: 92.39 %\n",
      "Epoch 32, duration: 131 s, loss: 0.0496, acc: 98.9946\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 33, duration: 131 s, loss: 0.0514, acc: 99.0761\n",
      "Accuracy of the network on the validation images: 92.39 %\n",
      "Epoch 34, duration: 131 s, loss: 0.0525, acc: 98.9674\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 35, duration: 131 s, loss: 0.0569, acc: 99.0489\n",
      "Accuracy of the network on the validation images: 92.64 %\n",
      "Epoch 36, duration: 131 s, loss: 0.0630, acc: 98.9266\n",
      "Accuracy of the network on the validation images: 92.64 %\n",
      "Epoch 37, duration: 131 s, loss: 0.0553, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.88 %\n",
      "Epoch 38, duration: 131 s, loss: 0.0570, acc: 99.0217\n",
      "Accuracy of the network on the validation images: 92.64 %\n",
      "Epoch 39, duration: 131 s, loss: 0.0588, acc: 99.1033\n",
      "Accuracy of the network on the validation images: 91.29 %\n",
      "Epoch 40, duration: 131 s, loss: 0.0519, acc: 99.0897\n",
      "Accuracy of the network on the validation images: 92.76 %\n",
      "Epoch 41, duration: 131 s, loss: 0.0566, acc: 98.9538\n",
      "Accuracy of the network on the validation images: 93.01 %\n",
      "Epoch 42, duration: 131 s, loss: 0.0537, acc: 98.9674\n",
      "Accuracy of the network on the validation images: 93.87 %\n",
      "Epoch 43, duration: 131 s, loss: 0.0393, acc: 99.0489\n",
      "Accuracy of the network on the validation images: 93.13 %\n",
      "Epoch 44, duration: 131 s, loss: 0.0549, acc: 99.0625\n",
      "Accuracy of the network on the validation images: 91.78 %\n",
      "Epoch 45, duration: 131 s, loss: 0.0541, acc: 99.0489\n",
      "Accuracy of the network on the validation images: 92.39 %\n",
      "Epoch 46, duration: 131 s, loss: 0.0520, acc: 98.9402\n",
      "Accuracy of the network on the validation images: 92.52 %\n",
      "Epoch 47, duration: 131 s, loss: 0.0522, acc: 99.0897\n",
      "Accuracy of the network on the validation images: 92.27 %\n",
      "Epoch 48, duration: 131 s, loss: 0.0534, acc: 98.9946\n",
      "Accuracy of the network on the validation images: 92.64 %\n",
      "Epoch 49, duration: 131 s, loss: 0.0544, acc: 98.9674\n",
      "Accuracy of the network on the validation images: 92.39 %\n",
      "Epoch 50, duration: 131 s, loss: 0.0532, acc: 99.0082\n",
      "Accuracy of the network on the validation images: 92.15 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "model_ft, training_losses, training_accs, test_accs = train_model(model_ft, criterion, optimizer, lrscheduler, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the image filename for all the prediction result.\n",
    "'''\n",
    "flatten = []\n",
    "for sublist in fname:\n",
    "    for item in sublist:\n",
    "        tmp_item = item.split(\"/\")\n",
    "        flatten.append(tmp_item[-1])\n",
    "\n",
    "'''\n",
    "This is the best confidence result together with the filename of the image.\n",
    "'''\n",
    "final = []\n",
    "for sublist in result:\n",
    "    for item in sublist:\n",
    "        final.append(item)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'filename': flatten,\n",
    "    'prediction': final\n",
    "})\n",
    "\n",
    "submission.to_csv('best.csv', index=False)\n",
    "        \n",
    "'''\n",
    "This is the confidence result for all the 196 classes together with the filename of the image.\n",
    "'''\n",
    "final2 = []\n",
    "for sublist in result2:\n",
    "    for item in sublist:\n",
    "        final2.append(item)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'filename': flatten,\n",
    "    'prediction': final2\n",
    "})\n",
    "\n",
    "submission.to_csv('confidence.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
